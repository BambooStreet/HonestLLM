# HonestLLM

Recently, generative language models (GPT, Bing, etc.) have been generating
hallucinations.
The source of the problem may be in the model's parameters or training method, but it's
difficult to fix it immediately.
So we saw a need for a tool that would allow users to verify the accuracy of model answers
and provide information about the reason behind them.

## Mission Statement
We are starting the HonestLLM open-source project.
The goal of the project is to develop technologies to determine the authenticity of the output
of AI models and to prevent hallucinations. Users will be able to visually check the reliability
of their answers using a fact-based database.
Through this, we will create a safe and reliable artificial intelligence environment and a more
certain and advanced future.

## Installation

## Usage

## Licenses

## Authors



## 
