# HonestLLM

Recently, generative language models (GPT, Bing, etc.) have been generating
hallucinations.
The source of the problem may be in the model's parameters or training method, but it's
difficult to fix it immediately.
So we saw a need for a tool that would allow users to verify the accuracy of model answers
and provide information about the reason behind them.

## Mission Statement
We are starting the HonestLLM open-source project.
The goal of the project is to develop technologies to determine the authenticity of the output
of AI models and to prevent hallucinations. Users will be able to visually check the reliability
of their answers using a fact-based database.
Through this, we will create a safe and reliable artificial intelligence environment and a more
certain and advanced future.

## Installation
system requirements

Python 3.8 or later

macOS, Windows are supported.


## Usage
It is waiting to be reviewed in the OPEN AI WAITLIST.

![installation](./img/installation.png)

## Licenses

  
Apache License 2.0  


## Authors

18011093 황성태  

18011543 박지환  

19010976 김민재  

19012022 홍석주  



## 
